{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paying 3rd party creators.</td>\n",
       "      <td>1</td>\n",
       "      <td>jeg3p2</td>\n",
       "      <td>https://www.reddit.com/r/startups/comments/jeg...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.603188e+09</td>\n",
       "      <td>I’m creating an e-commerce site (most likely w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm hosting a free Etsy event on Eventbrite!</td>\n",
       "      <td>0</td>\n",
       "      <td>jeftmx</td>\n",
       "      <td>https://www.reddit.com/r/startups/comments/jef...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.603187e+09</td>\n",
       "      <td>Come for an information/Q&amp;A event on Wednesday...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Leveraging old technology owned by someone els...</td>\n",
       "      <td>1</td>\n",
       "      <td>jef9o4</td>\n",
       "      <td>https://www.reddit.com/r/startups/comments/jef...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.603185e+09</td>\n",
       "      <td>Hi community friends - I have a question and I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Looking for resource to make a classifieds sty...</td>\n",
       "      <td>0</td>\n",
       "      <td>jee9u0</td>\n",
       "      <td>https://www.reddit.com/r/startups/comments/jee...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.603181e+09</td>\n",
       "      <td>Ideally something simple like Squarespace. May...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What health insurance does your startup use?</td>\n",
       "      <td>1</td>\n",
       "      <td>jeduys</td>\n",
       "      <td>https://www.reddit.com/r/startups/comments/jed...</td>\n",
       "      <td>6</td>\n",
       "      <td>1.603180e+09</td>\n",
       "      <td>Hey guys, we recently raised an institutional ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score      id  \\\n",
       "0                         Paying 3rd party creators.      1  jeg3p2   \n",
       "1       I'm hosting a free Etsy event on Eventbrite!      0  jeftmx   \n",
       "2  Leveraging old technology owned by someone els...      1  jef9o4   \n",
       "3  Looking for resource to make a classifieds sty...      0  jee9u0   \n",
       "4       What health insurance does your startup use?      1  jeduys   \n",
       "\n",
       "                                                 url  comms_num       created  \\\n",
       "0  https://www.reddit.com/r/startups/comments/jeg...          0  1.603188e+09   \n",
       "1  https://www.reddit.com/r/startups/comments/jef...          0  1.603187e+09   \n",
       "2  https://www.reddit.com/r/startups/comments/jef...          2  1.603185e+09   \n",
       "3  https://www.reddit.com/r/startups/comments/jee...          0  1.603181e+09   \n",
       "4  https://www.reddit.com/r/startups/comments/jed...          6  1.603180e+09   \n",
       "\n",
       "                                                body  subreddit  \n",
       "0  I’m creating an e-commerce site (most likely w...          1  \n",
       "1  Come for an information/Q&A event on Wednesday...          1  \n",
       "2  Hi community friends - I have a question and I...          1  \n",
       "3  Ideally something simple like Squarespace. May...          1  \n",
       "4  Hey guys, we recently raised an institutional ...          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data.\n",
    "df = pd.read_csv('../data/redditstartupproject.csv')\n",
    "\n",
    "# Check out first five rows.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['title', 'subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1891, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.524061\n",
       "1    0.475939\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CountVectorizer.\n",
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the vectorizer on our corpus.\n",
    "cvec.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the corpus.\n",
    "X_train_cv = cvec.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cv = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert X_train into a DataFrame.\n",
    "# We will not actually use this for modeling,\n",
    "# this is just to visualize what is happening\n",
    "# gives the count of each words\n",
    "X_train_df = pd.DataFrame(X_train_cv.todense(), \n",
    "                          columns=cvec.get_feature_names())\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test_cv.todense(), \n",
    "                          columns=cvec.get_feature_names())\n",
    "\n",
    "# plot top occuring words\n",
    "X_train_df.sum().sort_values(ascending=False).head(10).plot(kind='barh'); # all stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.525066\n",
       "1    0.474934\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.52381\n",
       "1    0.47619\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9675925925925926, 0.7730870712401056)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_train_df, y_train), lr.score(X_test_df, y_test)  # overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression  - with stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CountVectorizer.\n",
    "cvst = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "# Fit the vectorizer on our corpus.\n",
    "cvst.fit(X_train)\n",
    "\n",
    "# Transform the corpus.\n",
    "X_train_cvst = cvst.transform(X_train)\n",
    "X_test_cvst = cvst.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9609788359788359, 0.7493403693931399)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_cvst, y_train)\n",
    "lr.score(X_train_cvst, y_train), lr.score(X_test_cvst, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - with Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "# ss.fit(X_train_df)\n",
    "X_scaled = ss.fit_transform(X_train_df)\n",
    "\n",
    "X_test_sc = ss.transform(X_test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.712401055408971)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_scaled, y_train)\n",
    "lr.score(X_scaled, y_train), lr.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. CountVectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 2000, 3000, 4000, 5000\n",
    "# Minimum number of documents needed to include token: 2, 3\n",
    "# Maximum number of documents needed to include token: 90%, 95%\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [100, 300, 40, 500],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1, 1), (1, 2), (1, 3)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "\n",
    "gs = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                  pipe_params, # what parameters values are we searching?\n",
    "                  cv = 5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [100, 300, 40, 500],\n",
       "                         'cvec__min_df': [2, 3],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (1, 3)]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8432539682539683, 0.741424802110818)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train, y_train), gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7751207571088233"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the best score?\n",
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 500,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. tf-idf vectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "pipe_tvec = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_tvec_params = {\n",
    "    'tvec__max_features': [2000, 3000, 4000, 1000],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__ngram_range': [(1, 1), (1, 2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "gs_tvec = GridSearchCV(pipe_tvec, # what object are we optimizing?\n",
    "                        param_grid = pipe_tvec_params, # what parameters values are we searching?\n",
    "                        cv=5, verbose=1)# 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             param_grid={'tvec__max_features': [2000, 3000, 4000, 1000],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'tvec__stop_words': [None, 'english']},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tvec.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__max_features': 3000,\n",
       " 'tvec__ngram_range': (1, 2),\n",
       " 'tvec__stop_words': None}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tvec.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8015627390553626"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tvec.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8432539682539683, 0.741424802110818)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train, y_train), gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.7255936675461742)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "#                                                     y,\n",
    "#                                                     test_size=0.2,\n",
    "#                                                     stratify=y,\n",
    "#                                                     random_state=42)\n",
    "\n",
    "#instantiate model\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dt.fit(X_train_cv, y_train)\n",
    "\n",
    "dt.score(X_train_cv, y_train), dt.score(X_test_cv, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "et = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7830568487312307"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rf, X_train_cv, y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7890083710357791"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(et, X_train_cv, y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7949642646383844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': None, 'max_features': 'log2', 'n_estimators': 60}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [60, 75],\n",
    "    'max_depth': [None],\n",
    "    'max_features': ['auto', 'log2']\n",
    "}\n",
    "gs = GridSearchCV(rf, param_grid=params)\n",
    "gs.fit(X_train_cv, y_train)\n",
    "print(gs.best_score_) # cross val score!!\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7751120145127095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "params = {\n",
    "#     'n_estimators': [60, 75],\n",
    "#     'max_depth': [None],\n",
    "#     'max_features': ['auto', 'log2']\n",
    "}\n",
    "gs = GridSearchCV(rf, param_grid=params)\n",
    "gs.fit(X_train_cv, y_train)\n",
    "print(gs.best_score_) # cross val score!!\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [60, 75],\n",
    "    'max_depth': [None, 5, 3],\n",
    "    'max_features': ['auto', 'log2']\n",
    "}\n",
    "et = GridSearchCV(et, param_grid=params)\n",
    "et.fit(X_train_cv, y_train)\n",
    "print(gs.best_score_) # cross val score!!\n",
    "et.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
